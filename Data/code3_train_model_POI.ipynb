{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2eb99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "path = 'C:/Users/Administrator/practicumProject2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8ef1b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poi_id             int32\n",
      "timeWindow        object\n",
      "taxi_all           int32\n",
      "riderNum           int32\n",
      "temperature      float64\n",
      "precipitation    float64\n",
      "weatherCode        int32\n",
      "windSpeed        float64\n",
      "weekday           object\n",
      "holiday             bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "with open(path + 'trafficWeatherData.pickle', 'rb') as file:\n",
    "    allData = pickle.load(file)\n",
    "\n",
    "print(allData.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaceb91",
   "metadata": {},
   "source": [
    "# Compute a synthetic busyness measure \n",
    "\n",
    "Normalize the taxi busyness and subway busyneess, and compute the synthetic busyness. using the principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b0cf86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the taxi and subway busyness\n",
    "scaler = StandardScaler()\n",
    "columnsToScale = ['taxi_all','riderNum']\n",
    "columnsScaled = ['taxiAll_norm','riderNum_norm']\n",
    "temp_data = scaler.fit_transform(allData[columnsToScale])\n",
    "\n",
    "\n",
    "# compute the principal componet\n",
    "pca = PCA(n_components=1) # specify the enumber of principal component \n",
    "pca.fit(temp_data)\n",
    "pincipals = pca.transform(temp_data) # calculate the principal componenet\n",
    "\n",
    "\n",
    "# convert pricipal to list\n",
    "pincipals = pincipals.ravel()\n",
    "pincipals = pincipals.tolist()\n",
    "\n",
    "\n",
    "# put the busyness into the data set\n",
    "allData['busyness'] = pincipals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e227e",
   "metadata": {},
   "source": [
    "# Prepare data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9a9061b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "busyness         float64\n",
      "temperature      float64\n",
      "precipitation    float64\n",
      "windSpeed        float64\n",
      "poi_id            object\n",
      "timeWindow        object\n",
      "weekday           object\n",
      "holiday           object\n",
      "weatherCode       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# creeate the categorical feature list\n",
    "categorialFeatures = ['poi_id', 'timeWindow', 'weekday', 'holiday', 'weatherCode']\n",
    "# creeate the continuous feature list\n",
    "continuousFeatures = ['temperature', 'precipitation', 'windSpeed']\n",
    "\n",
    "# take out all the features\n",
    "allFeatures = continuousFeatures + categorialFeatures\n",
    "allData = allData[['busyness'] + allFeatures]\n",
    "\n",
    "# convert categorial features to string type\n",
    "for column in categorialFeatures:\n",
    "    allData[column] = allData[column].astype(str)\n",
    "\n",
    "allData.reset_index(drop=True, inplace=True)\n",
    "print(allData.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e157a1",
   "metadata": {},
   "source": [
    "Now, split data set into training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93609e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the target and input\n",
    "subData = allData.sample(frac = 1, replace=False, random_state = 44215)\n",
    "X = (subData.drop(columns=['busyness']))\n",
    "y = subData['busyness']\n",
    "\n",
    "# divid data to traning set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278199e7",
   "metadata": {},
   "source": [
    "Normalize the continuous columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a0a55f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a scaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# take out the continuous columns\n",
    "X_train_cont = X_train[continuousFeatures]\n",
    "X_test_cont = X_test[continuousFeatures]\n",
    "\n",
    "# normalize the continuous columns\n",
    "X_train_cont_scaled = scaler.fit_transform(X_train_cont)\n",
    "X_test_cont_scaled = scaler.transform(X_test_cont)\n",
    "\n",
    "# convert the normalized the continuous data to pandas data frame\n",
    "X_train_cont_scaled = pd.DataFrame(X_train_cont_scaled, columns = continuousFeatures)\n",
    "X_test_cont_scaled = pd.DataFrame(X_test_cont_scaled, columns = continuousFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2299c27",
   "metadata": {},
   "source": [
    "Encode the caregorial columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04649d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take out the categorical columns\n",
    "X_train_cat = X_train[categorialFeatures]\n",
    "X_test_cat = X_test[categorialFeatures]\n",
    "\n",
    "# encode the training data set\n",
    "X_train_cat_encoded = pd.get_dummies(X_train_cat, dtype=int, drop_first=True)\n",
    "\n",
    "# get the column names of the encoded training data set\n",
    "X_train_cat_encoded_columns = X_train_cat_encoded.columns\n",
    "\n",
    "# define an function to make the test data set be encoded consistently\n",
    "def uniformEncoding(X_train_cat_encoded_columns, X_new_cat):\n",
    "    # encode the new data set\n",
    "    X_new_cat_encoded = pd.get_dummies(X_new_cat, dtype=int, drop_first=True)\n",
    "    # take out the column names that new data misses\n",
    "    missed_cols = set(X_train_cat_encoded_columns) - set(X_new_cat_encoded.columns)\n",
    "    # add the missed columns to the new data set, assigning value 0 \n",
    "    for col in missed_cols:\n",
    "        X_new_cat_encoded[col] = 0\n",
    "    # select the columns that training data have in the same order\n",
    "    X_new_cat_encoded = X_new_cat_encoded[X_train_cat_encoded_columns]\n",
    "    return X_new_cat_encoded\n",
    "\n",
    "# apply the function to encode the categorical columns in testing data\n",
    "X_test_cat_encoded = uniformEncoding(X_train_cat_encoded_columns, X_test_cat)\n",
    "\n",
    "\n",
    "# reset row index\n",
    "X_train_cat_encoded.reset_index(drop=True, inplace=True)\n",
    "X_test_cat_encoded.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e71e3",
   "metadata": {},
   "source": [
    "Merge the normalized continuous columns and encoded categorical columns for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "703e7ed8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training data frame: (1911067, 268)\n",
      "The columns of training data frame:\n",
      "temperature       float64\n",
      "precipitation     float64\n",
      "windSpeed         float64\n",
      "poi_id_10           int32\n",
      "poi_id_100          int32\n",
      "                   ...   \n",
      "weatherCode_61      int32\n",
      "weatherCode_63      int32\n",
      "weatherCode_71      int32\n",
      "weatherCode_73      int32\n",
      "weatherCode_75      int32\n",
      "Length: 268, dtype: object \n",
      "\n",
      "The shape of test data frame: (477767, 268)\n",
      "The columns of test data frame:\n",
      "temperature       float64\n",
      "precipitation     float64\n",
      "windSpeed         float64\n",
      "poi_id_10           int32\n",
      "poi_id_100          int32\n",
      "                   ...   \n",
      "weatherCode_61      int32\n",
      "weatherCode_63      int32\n",
      "weatherCode_71      int32\n",
      "weatherCode_73      int32\n",
      "weatherCode_75      int32\n",
      "Length: 268, dtype: object \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate the whole training data set\n",
    "X_train = pd.concat([X_train_cont_scaled, X_train_cat_encoded], axis=1)\n",
    "print('The shape of training data frame:', X_train.shape)\n",
    "print('The columns of training data frame:')\n",
    "print(X_train.dtypes, '\\n')\n",
    "\n",
    "\n",
    "# generate the whole training data set\n",
    "X_test = pd.concat([X_test_cont_scaled, X_test_cat_encoded], axis=1)\n",
    "print('The shape of test data frame:', X_test.shape)\n",
    "print('The columns of test data frame:')\n",
    "print(X_test.dtypes, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b738d",
   "metadata": {},
   "source": [
    "Perform random forerst regression instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534220f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper-parameters\n",
    "n_estimators = 40\n",
    "max_depth = 40\n",
    "\n",
    "# create an instance of model\n",
    "rf_regressor = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, random_state = 4452, oob_score = False)\n",
    "\n",
    "# train the model on the training data\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# make prediction on the test data\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# calculate the mse\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "\n",
    "print('n_estimators = ', n_estimators)\n",
    "print('max_depth = ', max_depth)\n",
    "print('Mean Squared Error:', mse, end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ca444",
   "metadata": {},
   "source": [
    "Save the trained model on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf71e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dictionary to store the information for future prediction\n",
    "trainedModel = {'model':rf_regressor,\n",
    "                 'continuousFeatures':continuousFeatures,\n",
    "                 'categorialFeatures':categorialFeatures,\n",
    "                 'X_train_cat_encoded_columns':X_train_cat_encoded_columns,\n",
    "                 'scaler':scaler}\n",
    "\n",
    "# save the model on disk\n",
    "with open(path+'trainedModel_POI_40_40.pickle', 'wb') as file:\n",
    "    pickle.dump(trainedModel, file)\n",
    "\n",
    "# take out the values of busyness \n",
    "busyness = list(allData['busyness'])\n",
    "# save it on disk\n",
    "with open(path+'busyness_POI_40_40.pickle', 'wb') as file:\n",
    "    pickle.dump(busyness, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92500645",
   "metadata": {},
   "source": [
    "# Record the experiment result\n",
    "\n",
    "\n",
    "n_estimators =  30\n",
    "max_depth =  30\n",
    "Mean Squared Error: 0.3774785519905408\n",
    "Size = 150M\n",
    "\n",
    "n_estimators =  50\n",
    "max_depth =  40\n",
    "Mean Squared Error: 0.2860477142703501\n",
    "Size = 600M\n",
    "\n",
    "n_estimators =  40\n",
    "max_depth =  40\n",
    "Mean Squared Error: 0.28593275930093004\n",
    "Size = 480M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803021d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
